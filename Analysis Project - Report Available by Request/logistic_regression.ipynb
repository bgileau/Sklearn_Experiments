{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import sklearn.metrics\n",
    "import time\n",
    "\n",
    "import faiss  # KNN testing - python 3.5 only?\n",
    "from scipy.stats import mode\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os_dir = os.path.abspath('')\n",
    "os_dir_data = os.path.join(os_dir, r\"data/original/\")\n",
    "\n",
    "test_csv_path = os_dir_data + \"1000_test.csv\"\n",
    "train_csv_path = os_dir_data + \"1000_train.csv\"\n",
    "\n",
    "header_list = [\"# label\",\"f0\",\"f1\",\"f2\",\"f3\",\"f4\",\"f5\",\"f6\",\"f7\",\"f8\",\"f9\",\"f10\",\"f11\",\"f12\",\"f13\",\"f14\",\"f15\",\"f16\",\"f17\",\"f18\",\"f19\",\"f20\",\"f21\",\"f22\",\"f23\",\"f24\",\"f25\",\"f26\"]\n",
    "\n",
    "test_df = pd.read_csv(test_csv_path, names=header_list, skiprows=[0])  # skip row 0 because we are creating our own header\n",
    "train_df = pd.read_csv(train_csv_path, names=header_list, skiprows=[0])  # skip row 0 because we are creating our own header\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500000, 28)\n",
      "(7000000, 28)\n",
      "['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(test_df.shape)\n",
    "print(train_df.shape)\n",
    "\n",
    "####### scale data\n",
    "\n",
    "df_columns = list(test_df.columns)\n",
    "\n",
    "df_columns_features = df_columns\n",
    "df_columns_label = df_columns[0]\n",
    "df_columns_features.pop(0)\n",
    "\n",
    "\n",
    "print(df_columns_features)\n",
    "print(type(df_columns_features))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "test_df[df_columns_features] = scaler.fit_transform(test_df[df_columns_features])\n",
    "train_df[df_columns_features] = scaler.fit_transform(train_df[df_columns_features])\n",
    "\n",
    "####### end scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000000  # 1000000\n",
    "\n",
    "####### Separate the label from the normal dataframe\n",
    "\n",
    "test_subsample_df = test_df.sample(n=n_samples, random_state=1)\n",
    "train_subsample_df = train_df.sample(n=n_samples*2, random_state=1)\n",
    "\n",
    "test_subsample_label_arr = np.array(test_subsample_df[df_columns_label])\n",
    "test_subsample_df = test_subsample_df.drop(columns=df_columns_label)\n",
    "\n",
    "train_subsample_label_arr = np.array(train_subsample_df[df_columns_label])\n",
    "train_subsample_df = train_subsample_df.drop(columns=df_columns_label)\n",
    "#######\n",
    "\n",
    "\n",
    "###### Perform PCA on the subsample (if desired)\n",
    "do_perform_pca = False\n",
    "if do_perform_pca:\n",
    "    pca = PCA(.95, random_state=1)\n",
    "\n",
    "    pca.fit(train_subsample_df)  # fit pca on training set\n",
    "\n",
    "    print(f\"Total shape: {len(train_subsample_df.columns)}, PCA Component count: {pca.components_.shape}\")\n",
    "\n",
    "    # transform the test and train\n",
    "    train_subsample_df = pca.transform(train_subsample_df)\n",
    "    test_subsample_df = pca.transform(test_subsample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9611652253705477\n",
      "Took iterations: [[[18 15 14 12 11  8  5  5  2  2  1  1  1  1  1  1  1  1  1  1]\n",
      "  [17 16 14 12 11  8  5  5  2  2  1  1  1  1  1  1  1  1  1  1]\n",
      "  [18 15 14 12 11  8  5  5  2  2  1  1  1  1  1  1  1  1  1  1]\n",
      "  [18 16 14 12 11  8  7  5  2  2  1  1  1  1  1  1  1  1  1  1]\n",
      "  [18 15 14 12 11  8  5  5  2  2  1  1  1  1  1  1  1  1  1  1]]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8966    0.9138    0.9051    500182\n",
      "         1.0     0.9120    0.8945    0.9032    499818\n",
      "\n",
      "    accuracy                         0.9042   1000000\n",
      "   macro avg     0.9043    0.9042    0.9042   1000000\n",
      "weighted avg     0.9043    0.9042    0.9042   1000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_1_weight = 1\n",
    "label_0_weight = 1.4\n",
    "sample_weights_arr = np.where(train_subsample_label_arr == 1, label_1_weight, label_0_weight)\n",
    "\n",
    "clf_LR_original = LogisticRegressionCV(cv=5,Cs=20, penalty=\"l2\", random_state=1, solver=\"lbfgs\", max_iter=300, n_jobs=-1)\n",
    "clf_LR_original = clf_LR_original\n",
    "clf_LR_original.fit(train_subsample_df, train_subsample_label_arr, sample_weights_arr)\n",
    "\n",
    "report_LR_roc_auc = sklearn.metrics.roc_auc_score(y_true = test_subsample_label_arr, y_score=clf_LR_original.predict_proba(test_subsample_df)[:,1])\n",
    "print(f\"{report_LR_roc_auc}\")\n",
    "print(f\"Took iterations: {clf_LR_original.n_iter_}\")\n",
    "\n",
    "pred_labels = np.array(clf_LR_original.predict(test_subsample_df))\n",
    "report_LR = sklearn.metrics.classification_report(y_true = test_subsample_label_arr, y_pred = pred_labels, digits=4)\n",
    "print(report_LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 20, 28)\n",
      "(20, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.56683973e-01  4.61316873e-07  0.00000000e+00  6.06326060e-01\n",
      "  -1.53171068e-03  2.91854101e-02  1.93741005e+00 -4.41080356e-03\n",
      "   0.00000000e+00  2.08141500e-01  5.86287442e-01  3.51794256e-03\n",
      "  -2.03395700e-03  2.27484021e-01  2.63467741e-01  1.16778256e-03\n",
      "   3.20330193e-03  1.38655204e-01  1.08517481e-01  0.00000000e+00\n",
      "  -3.41727532e-07  5.67793882e-02  9.82394995e-02 -3.08125264e-01\n",
      "   2.45545328e-02  3.91007709e-03  1.00186753e+00]]\n"
     ]
    }
   ],
   "source": [
    "paths = clf_LR_reg.coefs_paths_\n",
    "\n",
    "paths = paths[1.0]\n",
    "print(paths.shape)\n",
    "\n",
    "paths_mean = np.mean(paths, axis=0)\n",
    "print(paths_mean.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(0, 28),paths_mean[1])\n",
    "# plt.xlabel('Training Sample Count')\n",
    "# plt.ylabel('AUC Values')\n",
    "# ax.set_xticks(n_samples_arr*2)\n",
    "plt.show()\n",
    "\n",
    "print(clf_LR_reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Logistic Regression CV\n",
      "On: l1 | liblinear\n",
      "0.9611302072419079 | 673.2462387084961\n",
      "On: l1 | saga\n",
      "0.961130162721902 | 532.5042934417725\n",
      "On: l2 | lbfgs\n",
      "0.9611186353683747 | 127.98631596565247\n",
      "On: l2 | liblinear\n",
      "0.9611198921245413 | 749.3991100788116\n",
      "On: l2 | sag\n",
      "0.96111867522838 | 559.5697376728058\n",
      "On: l2 | saga\n",
      "0.9611186672203789 | 407.2145240306854\n",
      "On: elasticnet | saga\n",
      "0.9611203079445964 | 2376.724447488785\n"
     ]
    }
   ],
   "source": [
    "############## Logistic Regression\n",
    "# Base LR CV \n",
    "\n",
    "class_weight_dict = {1: 1, 0:1}\n",
    "\n",
    "print(\"Starting Logistic Regression CV\")\n",
    "# Cross validation to find the best regularization strength first\n",
    "penalty_terms = [\"l1\",\"l2\",\"elasticnet\"]\n",
    "\n",
    "l1_ratio_arr = [.1,.3,.5,.7,.9]\n",
    "\n",
    "for pentalty_term in penalty_terms:\n",
    "    if pentalty_term == \"l2\":\n",
    "        solver_arr = [\"lbfgs\",\"liblinear\",\"sag\",\"saga\"]  # note: newton-cg has issues with convergence\n",
    "        for solver in solver_arr:\n",
    "            print(f\"On: {pentalty_term} | {solver}\")\n",
    "            time1 = time.time()\n",
    "            clf_LR_original = LogisticRegressionCV(cv=5,Cs=20, penalty=pentalty_term, random_state=1, class_weight=class_weight_dict, solver=solver, max_iter=200)  # TODO: test out different Cs values?\n",
    "\n",
    "            clf_LR_reg = clf_LR_original\n",
    "\n",
    "            clf_LR_reg.fit(train_subsample_df, train_subsample_label_arr)\n",
    "\n",
    "            report_LR_roc_auc = sklearn.metrics.roc_auc_score(y_true = test_subsample_label_arr, y_score=clf_LR_reg.predict_proba(test_subsample_df)[:,1])\n",
    "            print(f\"{report_LR_roc_auc} | {time.time()-time1}\")\n",
    "\n",
    "    elif pentalty_term == \"elasticnet\":\n",
    "        print(f\"On: {pentalty_term} | saga\")\n",
    "        time1 = time.time()\n",
    "        clf_LR_original = LogisticRegressionCV(cv=5,Cs=20, penalty=pentalty_term, random_state=1, class_weight=class_weight_dict, solver=\"saga\", max_iter=200, l1_ratios=l1_ratio_arr)  # TODO: test out different Cs values?\n",
    "\n",
    "        clf_LR_reg = clf_LR_original\n",
    "\n",
    "        clf_LR_reg.fit(train_subsample_df, train_subsample_label_arr)\n",
    "\n",
    "        report_LR_roc_auc = sklearn.metrics.roc_auc_score(y_true = test_subsample_label_arr, y_score=clf_LR_reg.predict_proba(test_subsample_df)[:,1])\n",
    "        print(f\"{report_LR_roc_auc} | {time.time()-time1}\")\n",
    "    else:\n",
    "        solver_arr = [\"liblinear\",\"saga\"]\n",
    "        for solver in solver_arr:\n",
    "            print(f\"On: {pentalty_term} | {solver}\")\n",
    "            time1 = time.time()\n",
    "            clf_LR_original = LogisticRegressionCV(cv=5,Cs=20, penalty=pentalty_term, random_state=1, class_weight=class_weight_dict, solver=solver, max_iter=200)  # TODO: test out different Cs values?\n",
    "\n",
    "            clf_LR_reg = clf_LR_original\n",
    "\n",
    "            clf_LR_reg.fit(train_subsample_df, train_subsample_label_arr)\n",
    "\n",
    "            report_LR_roc_auc = sklearn.metrics.roc_auc_score(y_true = test_subsample_label_arr, y_score=clf_LR_reg.predict_proba(test_subsample_df)[:,1])\n",
    "            print(f\"{report_LR_roc_auc} | {time.time()-time1}\")\n",
    "\n",
    "# LR_best_regularization = float(clf_LR_reg.C_)\n",
    "\n",
    "# print(f\"Best LR Regularization term: {LR_best_regularization}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1ab9e58c79c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mreport_LR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_subsample_label_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m# report_LR_roc_auc = sklearn.metrics.roc_auc_score(y_true = test_subsample_label_arr, y_score=clf_LR.predict_proba(test_subsample_df)[:,1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport_LR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred_labels' is not defined"
     ]
    }
   ],
   "source": [
    "# sample_weights_arr = np.where(train_subsample_label_arr == 1, 1, 1)  # make it weight of 2 if label = 1. Otherwise, make it 0.5.\n",
    "\n",
    "# clf_LR = clf_LR_original\n",
    "\n",
    "# # Fitting model\n",
    "# print(\"Fitting Model\")\n",
    "# clf_LR_original = LogisticRegressionCV(cv=5,Cs=20, penalty=\"l2\", random_state=1, solver=\"lbfgs\", max_iter=300, n_jobs=-1)\n",
    "\n",
    "\n",
    "pred_labels = np.array(clf_LR_original.predict(test_subsample_df))\n",
    "report_LR = sklearn.metrics.classification_report(y_true = test_subsample_label_arr, y_pred = pred_labels, digits=4)\n",
    "# report_LR_roc_auc = sklearn.metrics.roc_auc_score(y_true = test_subsample_label_arr, y_score=clf_LR.predict_proba(test_subsample_df)[:,1])\n",
    "print(report_LR)\n",
    "# print(report_LR_roc_auc)\n",
    "\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 1.8| Label 1: 0.1\n",
      "0.9596854631344851\n",
      "Label 0: 1.8| Label 1: 0.2\n",
      "0.9602796739412158\n",
      "Label 0: 1.8| Label 1: 0.3\n",
      "0.9605873625339831\n",
      "Label 0: 1.8| Label 1: 0.4\n",
      "0.9607761823550011\n",
      "Label 0: 1.8| Label 1: 0.5\n",
      "0.9609018260996484\n",
      "Label 0: 1.8| Label 1: 0.6\n",
      "0.9609885472671386\n",
      "Label 0: 1.8| Label 1: 0.7\n",
      "0.9610497154872432\n",
      "Label 0: 1.8| Label 1: 0.7999999999999999\n",
      "0.9610940289491144\n",
      "Label 0: 1.8| Label 1: 0.8999999999999999\n",
      "0.9611233971010056\n",
      "Label 0: 1.8| Label 1: 0.9999999999999999\n",
      "0.9611439144797241\n",
      "Label 0: 1.8| Label 1: 1.0999999999999999\n",
      "0.9611492397324296\n",
      "Label 0: 1.8| Label 1: 1.2\n",
      "0.9611628671182352\n",
      "Label 0: 1.8| Label 1: 1.3\n",
      "0.9611635874303306\n",
      "Label 0: 1.8| Label 1: 1.4\n",
      "0.9610875624082575\n",
      "Label 0: 1.8| Label 1: 1.5\n",
      "0.9611422796475073\n",
      "Label 0: 1.8| Label 1: 1.5999999999999999\n",
      "0.9610746724465498\n",
      "Label 0: 1.8| Label 1: 1.7\n",
      "0.9611234810090168\n",
      "Label 0: 1.8| Label 1: 1.9\n",
      "0.9611144308958177\n",
      "Label 0: 1.8| Label 1: 2.0\n",
      "0.961098531213711\n",
      "Label 0: 1.9| Label 1: 0.1\n",
      "0.9596373468281099\n",
      "Label 0: 1.9| Label 1: 0.2\n",
      "0.9602358137634044\n",
      "Label 0: 1.9| Label 1: 0.3\n",
      "0.9605488209968767\n",
      "Label 0: 1.9| Label 1: 0.4\n",
      "0.9607426837465627\n",
      "Label 0: 1.9| Label 1: 0.5\n",
      "0.960873022311832\n",
      "Label 0: 1.9| Label 1: 0.6\n",
      "0.9609643667319347\n",
      "Label 0: 1.9| Label 1: 0.7\n",
      "0.9610296264645815\n",
      "Label 0: 1.9| Label 1: 0.7999999999999999\n",
      "0.9610768042668323\n",
      "Label 0: 1.9| Label 1: 0.8999999999999999\n",
      "0.9611110820153739\n",
      "Label 0: 1.9| Label 1: 0.9999999999999999\n",
      "0.9611342356384418\n",
      "Label 0: 1.9| Label 1: 1.0999999999999999\n",
      "0.9611500925245426\n",
      "Label 0: 1.9| Label 1: 1.2\n",
      "0.9611605311699257\n",
      "Label 0: 1.9| Label 1: 1.3\n",
      "0.9611541162650759\n",
      "Label 0: 1.9| Label 1: 1.4\n",
      "0.961163393766305\n",
      "Label 0: 1.9| Label 1: 1.5\n",
      "0.9610927490369447\n",
      "Label 0: 1.9| Label 1: 1.5999999999999999\n",
      "0.9611424781755338\n",
      "Label 0: 1.9| Label 1: 1.7\n",
      "0.9610797319192202\n",
      "Label 0: 1.9| Label 1: 1.8\n",
      "0.9611242398211173\n",
      "Label 0: 1.9| Label 1: 2.0\n",
      "0.9611150899359049\n",
      "Label 0: 2.0| Label 1: 0.1\n",
      "0.9595917486220683\n",
      "Label 0: 2.0| Label 1: 0.2\n",
      "0.9601935514738047\n",
      "Label 0: 2.0| Label 1: 0.3\n",
      "0.9605113482919115\n",
      "Label 0: 2.0| Label 1: 0.4\n",
      "0.960710194870258\n",
      "Label 0: 2.0| Label 1: 0.5\n",
      "0.9608448500920993\n",
      "Label 0: 2.0| Label 1: 0.6\n",
      "0.9609401446687253\n",
      "Label 0: 2.0| Label 1: 0.7\n",
      "0.9610091812338726\n",
      "Label 0: 2.0| Label 1: 0.7999999999999999\n",
      "0.961059662984561\n",
      "Label 0: 2.0| Label 1: 0.8999999999999999\n",
      "0.9610969813015056\n",
      "Label 0: 2.0| Label 1: 0.9999999999999999\n",
      "0.961123392229005\n",
      "Label 0: 2.0| Label 1: 1.0999999999999999\n",
      "0.9611421433154894\n",
      "Label 0: 2.0| Label 1: 1.2\n",
      "0.961155084941204\n",
      "Label 0: 2.0| Label 1: 1.3\n",
      "0.9611620986341334\n",
      "Label 0: 2.0| Label 1: 1.4\n",
      "0.9611551718332156\n",
      "Label 0: 2.0| Label 1: 1.5\n",
      "0.961153197768954\n",
      "Label 0: 2.0| Label 1: 1.5999999999999999\n",
      "0.9610971814375321\n",
      "Label 0: 2.0| Label 1: 1.7\n",
      "0.9611425368715416\n",
      "Label 0: 2.0| Label 1: 1.8\n",
      "0.9610841426118045\n",
      "Label 0: 2.0| Label 1: 1.9\n",
      "0.9611248873652032\n",
      "Best L0: 1.8, Best L1: 1.3, AUC: 0.9611635874303306\n"
     ]
    }
   ],
   "source": [
    "sample_weights_arr = np.where(train_subsample_label_arr == 1, 1, 1)\n",
    "results_dict = dict.fromkeys(np.linspace(0,2,5))\n",
    "\n",
    "for keys, vals in results_dict.items():\n",
    "    results_dict[keys] = dict.fromkeys(np.linspace(0,2,5))\n",
    "\n",
    "max_val = 0\n",
    "best_l0 = 0\n",
    "best_l1 = 0\n",
    "for label_0_weight in np.linspace(1.8,2,3):\n",
    "    for label_1_weight in np.linspace(.1,2,20):\n",
    "\n",
    "        if label_0_weight == label_1_weight or label_0_weight < label_1_weight - .5:\n",
    "            continue  # will be the same each time\n",
    "        print(f\"Label 0: {label_0_weight}| Label 1: {label_1_weight}\")\n",
    "        sample_weights_arr = np.where(train_subsample_label_arr == 1, label_1_weight, label_0_weight)\n",
    "        clf_LR_original.fit(train_subsample_df, train_subsample_label_arr, sample_weights_arr)\n",
    "        probs = clf_LR_original.predict_proba(test_subsample_df)[:,1]\n",
    "        report_LR_roc_auc = sklearn.metrics.roc_auc_score(y_true = test_subsample_label_arr, y_score=probs)\n",
    "\n",
    "        # results_dict[label_0_weight][label_1_weight] = report_LR_roc_auc\n",
    "\n",
    "        if report_LR_roc_auc > max_val:\n",
    "            max_val = report_LR_roc_auc\n",
    "            best_l0 = label_0_weight\n",
    "            best_l1 = label_1_weight\n",
    "\n",
    "        print(f\"{report_LR_roc_auc}\")\n",
    "print(f\"Best L0: {best_l0}, Best L1: {best_l1}, AUC: {max_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19149,) (180851,)\n",
      "[1.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9138    0.8930    0.9033    100105\n",
      "         1.0     0.8951    0.9156    0.9052     99895\n",
      "\n",
      "    accuracy                         0.9043    200000\n",
      "   macro avg     0.9045    0.9043    0.9042    200000\n",
      "weighted avg     0.9045    0.9043    0.9042    200000\n",
      "\n",
      "0.9596681474341323\n"
     ]
    }
   ],
   "source": [
    "# Adjust weights based on prediction accuracy, not predict proba:\n",
    "\n",
    "# incorrect_indicies = np.where(pred_labels != train_subsample_label_arr)\n",
    "# correct_indicies = np.where(pred_labels == train_subsample_label_arr)\n",
    "\n",
    "# sample_weights_arr = np.where(train_subsample_label_arr == 1, 1, 1)\n",
    "\n",
    "clf_LR = clf_LR_original\n",
    "\n",
    "sample_weights_arr = np.ones(n_samples*2)\n",
    "\n",
    "diff_arr = pred_labels - test_subsample_label_arr  # if same, then 0 -> if correct, then zero.\n",
    "\n",
    "incorrect_indicies = np.array(np.nonzero(diff_arr)).ravel()\n",
    "correct_indicies = np.where(diff_arr == 0)[0]\n",
    "\n",
    "print(incorrect_indicies.shape, correct_indicies.shape)\n",
    "\n",
    "# sample_weights_arr[correct_indicies] = sample_weights_arr[correct_indicies] - .5\n",
    "# sample_weights_arr[incorrect_indicies] = sample_weights_arr[incorrect_indicies] + .5\n",
    "\n",
    "sample_weights_arr.put(correct_indicies,.5)\n",
    "sample_weights_arr.put(incorrect_indicies,1.5)\n",
    "\n",
    "# sample_weights_arr[correct_indicies] = .5\n",
    "# sample_weights_arr[incorrect_indicies] = 1.5\n",
    "\n",
    "print(sample_weights_arr)\n",
    "\n",
    "clf_LR_adj = LogisticRegression(C=LR_best_regularization,penalty=\"l2\", random_state=1)\n",
    "clf_LR_adj.fit(train_subsample_df, train_subsample_label_arr.ravel(), sample_weight=sample_weights_arr)\n",
    "pred_labels_adj = np.array(clf_LR_adj.predict(test_subsample_df))\n",
    "\n",
    "report_LR_roc_auc = sklearn.metrics.roc_auc_score(y_true = test_subsample_label_arr, y_score=clf_LR_adj.predict_proba(test_subsample_df)[:,1])\n",
    "print(report_LR)\n",
    "print(report_LR_roc_auc)\n",
    "\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Prob a testing below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 28 features per sample; expecting 27",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e4d58b547ab9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;31m###### test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;31m# predict_proba attempt to optimize sample_weights for edge prediction cases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m \u001b[0mpred_probs_LR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_LR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# check on train data to reweight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_probs_LR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1467\u001b[0m                                                 self.solver == 'liblinear')))\n\u001b[0;32m   1468\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0movr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1469\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1470\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m             \u001b[0mdecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36m_predict_proba_lr\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mmulticlass\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mhandled\u001b[0m \u001b[0mby\u001b[0m \u001b[0mnormalizing\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mover\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \"\"\"\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m         \u001b[0mexpit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 289\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 28 features per sample; expecting 27"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # predict_proba attempt to optimize sample_weights for edge prediction cases\n",
    "# pred_probs_LR = clf_LR.predict_proba(train_df)  # check on train data to reweight\n",
    "# print(pred_probs_LR.shape)\n",
    "\n",
    "# pred_probs_LR_diff = np.abs(np.diff(pred_probs_LR, axis=1))\n",
    "\n",
    "# # High values need to lose weight. Low values need to gain weight.\n",
    "# min_weight = .4\n",
    "# max_weight = .6\n",
    "\n",
    "# pred_probs_LR_diff_proportion = (1 - pred_probs_LR_diff) * (max_weight-min_weight) + min_weight\n",
    "# pred_probs_LR_diff_proportion = pred_probs_LR_diff_proportion.ravel()\n",
    "\n",
    "# print(pred_probs_LR_diff_proportion.shape)\n",
    "\n",
    "# print(\"new range \",np.min(pred_probs_LR_diff_proportion), np.max(pred_probs_LR_diff_proportion))\n",
    "\n",
    "# clf_LR = LogisticRegression(C=LR_best_regularization,penalty=\"l2\", random_state=1)\n",
    "# clf_LR.fit(train_df, train_label_arr.ravel(), sample_weight=pred_probs_LR_diff_proportion)\n",
    "# pred_labels_adj = np.array(clf_LR.predict(test_df))\n",
    "\n",
    "# report_LR_adj = sklearn.metrics.classification_report(y_true = test_label_arr, y_pred = pred_labels_adj, digits=4)\n",
    "# print(report_LR_adj)\n",
    "\n",
    "# ############## \n",
    "\n",
    "# ###### test\n",
    "# # predict_proba attempt to optimize sample_weights for edge prediction cases\n",
    "# pred_probs_LR = clf_LR.predict_proba(train_df)  # check on train data to reweight\n",
    "# print(pred_probs_LR.shape)\n",
    "\n",
    "# pred_probs_LR_diff = np.abs(np.diff(pred_probs_LR, axis=1))\n",
    "\n",
    "# # High values need to lose weight. Low values need to gain weight.\n",
    "# min_weight = .3\n",
    "# max_weight = .7\n",
    "\n",
    "# pred_probs_LR_diff_proportion = (1 - pred_probs_LR_diff) * (max_weight-min_weight) + min_weight\n",
    "# pred_probs_LR_diff_proportion = pred_probs_LR_diff_proportion.ravel()\n",
    "\n",
    "# print(pred_probs_LR_diff_proportion.shape)\n",
    "\n",
    "# print(\"new range \",np.min(pred_probs_LR_diff_proportion), np.max(pred_probs_LR_diff_proportion))\n",
    "\n",
    "# clf_LR = LogisticRegression(C=LR_best_regularization,penalty=\"l2\", random_state=1)\n",
    "# clf_LR.fit(train_df, train_label_arr.ravel(), sample_weight=pred_probs_LR_diff_proportion)\n",
    "# pred_labels_adj = np.array(clf_LR.predict(test_df))\n",
    "\n",
    "# report_LR_adj = sklearn.metrics.classification_report(y_true = test_label_arr, y_pred = pred_labels_adj, digits=4)\n",
    "# print(report_LR_adj)\n",
    "\n",
    "# ######\n",
    "\n",
    "# ###### test\n",
    "# # predict_proba attempt to optimize sample_weights for edge prediction cases\n",
    "# pred_probs_LR = clf_LR.predict_proba(train_df)  # check on train data to reweight\n",
    "# print(pred_probs_LR.shape)\n",
    "\n",
    "# pred_probs_LR_diff = np.abs(np.diff(pred_probs_LR, axis=1))\n",
    "\n",
    "# # High values need to lose weight. Low values need to gain weight.\n",
    "# min_weight = .2\n",
    "# max_weight = .8\n",
    "\n",
    "# pred_probs_LR_diff_proportion = (1 - pred_probs_LR_diff) * (max_weight-min_weight) + min_weight\n",
    "# pred_probs_LR_diff_proportion = pred_probs_LR_diff_proportion.ravel()\n",
    "\n",
    "# print(pred_probs_LR_diff_proportion.shape)\n",
    "\n",
    "# print(\"new range \",np.min(pred_probs_LR_diff_proportion), np.max(pred_probs_LR_diff_proportion))\n",
    "\n",
    "# clf_LR = LogisticRegression(C=LR_best_regularization,penalty=\"l2\", random_state=1)\n",
    "# clf_LR.fit(train_df, train_label_arr.ravel(), sample_weight=pred_probs_LR_diff_proportion)\n",
    "# pred_labels_adj = np.array(clf_LR.predict(test_df))\n",
    "\n",
    "# report_LR_adj = sklearn.metrics.classification_report(y_true = test_label_arr, y_pred = pred_labels_adj, digits=4)\n",
    "# print(report_LR_adj)\n",
    "\n",
    "# ######\n",
    "\n",
    "# ###### test\n",
    "# # predict_proba attempt to optimize sample_weights for edge prediction cases\n",
    "# pred_probs_LR = clf_LR.predict_proba(train_df)  # check on train data to reweight\n",
    "# print(pred_probs_LR.shape)\n",
    "\n",
    "# pred_probs_LR_diff = np.abs(np.diff(pred_probs_LR, axis=1))\n",
    "\n",
    "# # High values need to lose weight. Low values need to gain weight.\n",
    "# min_weight = .1\n",
    "# max_weight = .9\n",
    "\n",
    "# pred_probs_LR_diff_proportion = (1 - pred_probs_LR_diff) * (max_weight-min_weight) + min_weight\n",
    "# pred_probs_LR_diff_proportion = pred_probs_LR_diff_proportion.ravel()\n",
    "\n",
    "# print(pred_probs_LR_diff_proportion.shape)\n",
    "\n",
    "# print(\"new range \",np.min(pred_probs_LR_diff_proportion), np.max(pred_probs_LR_diff_proportion))\n",
    "\n",
    "# clf_LR = LogisticRegression(C=LR_best_regularization,penalty=\"l2\", random_state=1)\n",
    "# clf_LR.fit(train_df, train_label_arr.ravel(), sample_weight=pred_probs_LR_diff_proportion)\n",
    "# pred_labels_adj = np.array(clf_LR.predict(test_df))\n",
    "\n",
    "# report_LR_adj = sklearn.metrics.classification_report(y_true = test_label_arr, y_pred = pred_labels_adj, digits=4)\n",
    "# print(report_LR_adj)\n",
    "\n",
    "# ######\n",
    "\n",
    "###### test\n",
    "# predict_proba attempt to optimize sample_weights for edge prediction cases\n",
    "pred_probs_LR = clf_LR.predict_proba(train_df)  # check on train data to reweight\n",
    "print(pred_probs_LR.shape)\n",
    "\n",
    "pred_probs_LR_diff = np.abs(np.diff(pred_probs_LR, axis=1))\n",
    "\n",
    "# High values need to lose weight. Low values need to gain weight.\n",
    "min_weight = 0\n",
    "max_weight = 1\n",
    "\n",
    "pred_probs_LR_diff_proportion = (1 - pred_probs_LR_diff) * (max_weight-min_weight) + min_weight\n",
    "pred_probs_LR_diff_proportion = pred_probs_LR_diff_proportion.ravel()\n",
    "\n",
    "print(pred_probs_LR_diff_proportion.shape)\n",
    "\n",
    "print(\"new range \",np.min(pred_probs_LR_diff_proportion), np.max(pred_probs_LR_diff_proportion))\n",
    "\n",
    "clf_LR = LogisticRegression(C=LR_best_regularization,penalty=\"l2\", random_state=1)\n",
    "clf_LR.fit(train_df, train_label_arr.ravel(), sample_weight=pred_probs_LR_diff_proportion)\n",
    "pred_labels_adj = np.array(clf_LR.predict(test_df))\n",
    "\n",
    "report_LR_adj = sklearn.metrics.classification_report(y_true = test_label_arr, y_pred = pred_labels_adj, digits=4)\n",
    "print(report_LR_adj)\n",
    "\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Logistic Regression CV\n",
      "On: 100000\n",
      "0.9595364475852252\n",
      "On: 500000\n",
      "0.9607344402369297\n",
      "On: 1000000\n",
      "0.9611188512724034\n",
      "On: 1500000\n",
      "0.9611260629123828\n",
      "On: 2000000\n",
      "0.9612023241830305\n",
      "On: 2500000\n",
      "0.9612137580686269\n",
      "On: 3000000\n",
      "0.9611740632881919\n",
      "On: 3500000\n"
     ]
    }
   ],
   "source": [
    "# Find the optimal sample size (done - 1 million test/2 mill train)\n",
    "\n",
    "class_weight_dict = {1: 1, 0:1}\n",
    "\n",
    "print(\"Starting Logistic Regression CV\")\n",
    "# Cross validation to find the best regularization strength first\n",
    "clf_LR_original = LogisticRegressionCV(cv=5,Cs=20, penalty=\"l2\", random_state=1, class_weight=class_weight_dict, n_jobs=-1)  # TODO: test out different Cs values?\n",
    "\n",
    "n_samples_arr = [100000,500000,1000000,1500000,2000000,2500000, 3000000,3500000]\n",
    "# n_samples_arr = [1000,2000,3000]\n",
    "\n",
    "n_samples_arr = np.array(n_samples_arr)\n",
    "time_arr = []\n",
    "auc_arr = []\n",
    "for n_samples in n_samples_arr:\n",
    "    ####### Separate the label from the normal dataframe\n",
    "    test_subsample_df = test_df.sample(n=n_samples, random_state=1)\n",
    "    train_subsample_df = train_df.sample(n=n_samples*2, random_state=1)\n",
    "\n",
    "    test_subsample_label_arr = np.array(test_subsample_df[df_columns_label])\n",
    "    test_subsample_df = test_subsample_df.drop(columns=df_columns_label)\n",
    "\n",
    "    train_subsample_label_arr = np.array(train_subsample_df[df_columns_label])\n",
    "    train_subsample_df = train_subsample_df.drop(columns=df_columns_label)\n",
    "    #######\n",
    "\n",
    "    clf_LR = clf_LR_original\n",
    "\n",
    "    time1 = time.time()\n",
    "\n",
    "    print(f\"On: {n_samples}\")\n",
    "\n",
    "    clf_LR.fit(train_subsample_df, train_subsample_label_arr)\n",
    "    # LR_best_regularization = float(clf_LR.C_)\n",
    "    # print(f\"Best LR Regularization term: {LR_best_regularization}\")\n",
    "\n",
    "    # # Fitting model\n",
    "    # print(\"Fitting Model\")\n",
    "    # clf_LR = LogisticRegression(C=LR_best_regularization,penalty=\"l2\", random_state=1)  # class weights if fed in here act as multipliers for the sample weights\n",
    "    # clf_LR.fit(train_subsample_df, train_subsample_label_arr.ravel())\n",
    "    pred_labels = np.array(clf_LR.predict(test_subsample_df))\n",
    "\n",
    "    # report_LR = sklearn.metrics.classification_report(y_true = test_subsample_label_arr, y_pred = pred_labels, digits=4)\n",
    "    report_LR_roc_auc = sklearn.metrics.roc_auc_score(y_true = test_subsample_label_arr, y_score=clf_LR.predict_proba(test_subsample_df)[:,1])\n",
    "    # print(report_LR)\n",
    "    print(report_LR_roc_auc)\n",
    "\n",
    "    auc_arr.append(report_LR_roc_auc)\n",
    "    time_arr.append(time.time()-time1)\n",
    "\n",
    "    ###################\n",
    "\n",
    "#print(n_samples_arr, time_arr, auc_arr)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(n_samples_arr*2, time_arr)\n",
    "plt.xlabel('Training Sample Count')\n",
    "plt.ylabel('Time to Run')\n",
    "ax.set_xticks(n_samples_arr*2)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(n_samples_arr*2, auc_arr)\n",
    "plt.xlabel('Training Sample Count')\n",
    "plt.ylabel('AUC Values')\n",
    "ax.set_xticks(n_samples_arr*2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
